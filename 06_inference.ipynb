{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook : Inference\n",
    "\n",
    "We've spent a lot of time in section getting comfortable with working with data in Python. We've wrangled, explored, and visualized. We've answered questions...but haven't done so in a statistically rigorous way. That stops now. In this workbook, we'll work through distributions and regression. Then, next week, you'll have the opportunity to carry out the analysis discussed during our in-class case study discussion.\n",
    "\n",
    "To get started **run the cell below to import the packages we'll use in this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Distributions\n",
    "\n",
    "Note: This section includes content from a [section workbook](https://github.com/robloughnan/cogs108_w19-section/blob/master/Week_7/Distributions_and_Ordinary_Least_Squares.ipynb) developed and authored by **Rob Laughnan**, a current COGS PhD stduent.   \n",
    "\n",
    "In this section we will look at different distributions and plot them out. Additionally, we will consider what types of processes might have generated each of these distributions.\n",
    "\n",
    "The module stats in scipy has many useful distributions, below import from this module each of the following distributions:\n",
    "\n",
    "- uniform\n",
    "- norm\n",
    "- bernoulli\n",
    "- poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import these modules from stats in scipy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these distributions are represented as classes by `scipy`. In order to generate a random sample from each of these distributions, we'll use the method `rvs`. We will generate 1000 samples for each distribution and plot them out using a histogram. The argument you'll define specifies the number of samples (the argument in the rvs method is `size`). \n",
    "\n",
    "A reminder that you can alwas look at the documentation when using something new or unfamiliar. In order to do this use `?` (e.g. `uniform?`) and look at the methods section of the documentation to see what arguments `rvs` takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "**Use `uniform.rvs` and assign its output to the variable `samples`. In this function, specify the location parameter to be 100, set the scale to be 20, and the sample size to be 1000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(samples)==1000\n",
    "assert isinstance(samples, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, generate a histogram of this distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, step through the same process to generate a histogram, but this time for a Normal distribution of the same sample size with a mean of 100 and standard deviation of 15.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do the same but for a Bernoulli distribution with a probabiliy of 0.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "**Finally, create a histogram of a Poisson distribution with mean 4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Discuss with your classmates which distribution matches each of the data generation processes you just generated?\n",
    "\n",
    "- Toss of a coin \n",
    "- The serial number of any dollar bill chosen at random\n",
    "- The number of hurricanes in a given year\n",
    "- IQ scores in the general population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Data Simulation & Correlation\n",
    "\n",
    "This week, we're just trying to understand the principles behind inference, so we're going to simulate data. However, next week, you'll work with a real dataset to answer a question using inference.\n",
    "\n",
    "We are going to simulate idealized age and income data using these distributions. First **create an array `age` that has 1000 random samples of a uniform distribution from 0 to 80 (Hint: you can use multiplication to scale the interval you desire). Additionally, create another array, `income`, with 1000 samples from a normal distribution that has a mean of 30 and a standard-deviation of 10 (working in units of 1000s of dollars).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(age) == 1000\n",
    "assert round(np.mean(income)) == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a scatter plot of these two arrays and calculate thier Pearson correlation coefficient and associated p-value - you will need to import `pearsonr` from scipy.stats to do this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pearsonr(age, income)[0] < 0.1\n",
    "assert pearsonr(age, income)[1] > 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you have found that these two arrays are weakly and non-significantly (p>0.05) correlated, this is because the two arrays were created independantly. Next, we are going to look at simulating data in which income is related to age. In order to do this, **set up income so that its mean is no longer a single value but instead is a function of age, specifically it is is 0.2*age. Create the same plot and calcualte the pearson correlation as above with income defined in this new way.**\n",
    "\n",
    "Note that `np.clip` will be useful to ensure that you do not get anyone with negative income.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pearsonr(age, income)[0] > 0.1\n",
    "assert pearsonr(age, income)[1] < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III : OLS\n",
    "\n",
    "We will now perform ordinary least squares (OLS) to see if we can recover the coefficient we used to simulate the data. For this we will use the library `statsmodels` (imported below for you). **Look at the method OLS and by using the example in its documenation or the see if you can fit a linear model between the arrays age and income to see if you can recover the coefficient we used to generate the data. Call the model you fit `results`.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've fit your model, you should be able to **use the method `summary` on your `results` variable to see if you've recovered the coefficient we previously specified during model generation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
