{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook : Text Analysis\n",
    "\n",
    "In this workbook we'll do a short text analysis so that you start to become familiar with the packages and tools available to you in Python to work with text data. Nothing here will be very in-depth - it's supposed to be able to be completed in an hour after all. But, it will give you a starting point for your final assignment and projects, should you want to analyze text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Question\n",
    "In this short project, we're going to answer the question: *For each presidential inauguration, which word is most unique?* \n",
    "\n",
    "To do this, we'll use the text from each Inaugural address in American history and carry out a TF-IDF analaysis.\n",
    "\n",
    "Secondarily, we'll think about whether these words make sense in the context of the history at the time and visualize words uniqueness over the course of history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Setup & Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook uses a number of different functions across multiple packages. **Run the following code cell and take a look at each package we'll be using below. Make sure you understand what the package is used for. Google anything that you're not familiar with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import nltk package \n",
    "# NLTK provides support for a wide variety of text processing tasks: \n",
    "# tokenization, stemming, proper name identification, part of speech identification, etc. \n",
    "#   PennTreeBank word tokenizer \n",
    "#   English language stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# scikit-learn imports\n",
    "#   TF-IDF Vectorizer that first removes widely used words in the dataset and then transforms test data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# import re for regular expression\n",
    "import re\n",
    "\n",
    "## seaborn for plotting\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.2, style=\"white\")\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# set plotting size parameter\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started on your text analysis using the `nltk` package, you'll want to **download the NLTK English tokenizer ('punkt') and stopwords of all languages ('stopwords') from `nltk`**. To determine what code you'll need to do this, you can explore the `download` method [here](https://www.nltk.org/) or their book [here](http://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have downloaded a few of the datasets you'll need, **import the `inaugural` dataset from `nltk.corpus`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all is working well, the following cell should display the files included in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is one file from each address. And, you'll note that the filename includes the year of each address. We'll want to use that address later, so **write code that extracts each year from the filename and stores it as a list. Assign this list to the variable `years`.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(years) == len(inaugural.fileids())\n",
    "assert years[1] == '1793'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these addresses. We'll pick a short one - Washington's *second* address. **Run the code below to take a look.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.raw('1793-Washington.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that there are some new line characters, as well as a colon, some commas, some periods. We're really only interested in the words though for TF-IDF, so let's remove all punctuation. **Write code that returns a list, where each element includes the text as above, but with punctuation removed and each word separated by a space. Assign this to the variable `text`. You may also want to convert all words to lowercase so that \"Constitution\" is not different than \"constitution\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done this correctly and you **run the following cell, all punctuation should be stripped from the text, so that you only see the words from Washington's second address, separated by spaces.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, you now have a dataset ready for analysis by TF-IDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started on your TF-IDF analysis, you'll first want to **create a `TfidfVectorizer` object to transform your text data into vectors. Assign this `TfidfVectorizer` object to `tfidf`.**\n",
    "\n",
    "In this object, you'll need to **pass five arguments to initialize `tfidf`**: \n",
    "* set to apply TF scaling: `sublinear_tf=True`\n",
    "* analyze at the word-level: `analyzer='word'`\n",
    "* set maximum number of unique words: ` max_features=2000`\n",
    "* specify that you want to tokenize the data using the word_tokenizer from NLTK: `tokenizer=word_tokenize`\n",
    "* remove English language stop words: `stop_words=stopwords.words(\"english\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tfidf.analyzer == 'word'\n",
    "assert tfidf.max_features == 2000\n",
    "assert tfidf.tokenizer == word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to calculate TF-IDF for words across our corpus of Inaugural addresses! **To do this, generate a `DataFrame` using the `tfidf.fit_transform` function to calculate TF-IDF on your `text` variable. Assing this to the variable `inaug_tfidf`. Be sure that your index here is the year of the address and the columns are named with the columns of the words the values represent. The `get_feature_names` method may help you accomplish this.** There are a lot of steps in here. If you're struggling, notify a TA/IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(inaug_tfidf.index)==len(years)\n",
    "assert len(inaug_tfidf.columns)==2000\n",
    "assert inaug_tfidf.shape == (56, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost there. We now have a DataFrame that includes the TF-IDF for the top 2000 words in our corpus! **Now, you'll want to extract the most unique word from each address. Assign this information (most likely a Series object) to the variable `most_unique`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look through this list of most unique words over time. Do they make sense based on what you know about American history? Do any surprise you?\n",
    "\n",
    "With that part of our Analysis done, one thing that stuck out to me in this list is the fact that \"british\" was the most unique word to the 1813 inaugural address. This made sense to me - it was early in American history and we had only recently left British rule. But, I was curious to see whether or not 'british' would show up uniquely (albeit less uniquely) in any later addresses. **Generate a line plot that plots the TF-IDF for the word \"british\" on the y-axis. Plot year on the x-axis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that over time \"british\" peaked in inaugural addresses at a few interesting points throughout history. What about some other words?\n",
    "\n",
    "Using a similar approach, **plot TF-IDF for \"british\", \"america\", and \"war\". Take a look at the trends over time. Feel free to look at other words' trends over time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mentioning America happened frequently in the country's infancy, but then became less common. This trend has seen an increase in frequency in recent years, though. War appears to pop up in its TF-IDF throughout America's history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all analysis, TF-IDF is not without its limitations. Let's take a look at how our results change if we change the `max_features` result in our analysis above to include 4000 words (rather than 2000). **Redo the analysis to 1) calculate TF-IDF for these 4000 words, 2) identify the word with the highest TF-IDF in each year (assignt his to `most_unique_4000`, and 3) generate a dataframe with the most common word from each analysis.Then, take a look to see how changing one argument in your analysis can affect your results! Finally, you can regenerate line plots if you're interseted to see how your plots have changed in this new analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
